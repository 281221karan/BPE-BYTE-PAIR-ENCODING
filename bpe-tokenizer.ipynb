{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T09:54:45.664367Z",
     "iopub.status.busy": "2025-05-13T09:54:45.663603Z",
     "iopub.status.idle": "2025-05-13T09:54:52.709697Z",
     "shell.execute_reply": "2025-05-13T09:54:52.708973Z",
     "shell.execute_reply.started": "2025-05-13T09:54:45.664340Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (8.1.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (7.34.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (5.7.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (75.1.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.50)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fsspec\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed fsspec-2024.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets tqdm ipywidgets\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T11:21:18.638019Z",
     "iopub.status.busy": "2025-05-12T11:21:18.637184Z",
     "iopub.status.idle": "2025-05-12T11:21:20.819214Z",
     "shell.execute_reply": "2025-05-12T11:21:20.818503Z",
     "shell.execute_reply.started": "2025-05-12T11:21:18.637992Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ds = load_dataset(\"benlehrburger/college-text-corpus\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T11:21:20.821880Z",
     "iopub.status.busy": "2025-05-12T11:21:20.820057Z",
     "iopub.status.idle": "2025-05-12T11:21:21.175844Z",
     "shell.execute_reply": "2025-05-12T11:21:21.174679Z",
     "shell.execute_reply.started": "2025-05-12T11:21:20.821857Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import trange, tqdm\n",
    "raw_text = \"\"\n",
    "for i in trange(len(ds[\"train\"])):\n",
    "    raw_text += ds[\"train\"][i][\"text\"]\n",
    "raw_text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T11:21:21.177794Z",
     "iopub.status.busy": "2025-05-12T11:21:21.177513Z",
     "iopub.status.idle": "2025-05-12T11:21:21.183953Z",
     "shell.execute_reply": "2025-05-12T11:21:21.183016Z",
     "shell.execute_reply.started": "2025-05-12T11:21:21.177771Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "len(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T09:54:56.703819Z",
     "iopub.status.busy": "2025-05-13T09:54:56.703379Z",
     "iopub.status.idle": "2025-05-13T09:55:00.365112Z",
     "shell.execute_reply": "2025-05-13T09:55:00.364289Z",
     "shell.execute_reply.started": "2025-05-13T09:54:56.703796Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "170fad69023142789004ca177286952c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad88ba315d34b6b836c6679989884a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.csv:   0%|          | 0.00/33.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da7f5d78c7340e38ba202bc585e6227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.csv:   0%|          | 0.00/8.74M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66164e48d2ce4a27bb1440dc1c5063cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/22930 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12937b3735ea4120a8c5bfab39aa1a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'abstract', 'label'],\n",
       "        num_rows: 22930\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'abstract', 'label'],\n",
       "        num_rows: 5732\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds1 = load_dataset(\"Ateeqq/AI-and-Human-Generated-Text\")\n",
    "ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T09:55:00.366730Z",
     "iopub.status.busy": "2025-05-13T09:55:00.366355Z",
     "iopub.status.idle": "2025-05-13T10:05:21.330020Z",
     "shell.execute_reply": "2025-05-13T10:05:21.314735Z",
     "shell.execute_reply.started": "2025-05-13T09:55:00.366710Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7246925ef03a43b5ab5f9570c7a0c4b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22930 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nThis study focuses on the epigenetic inheritance of circadian period in clonal cells. In recent years, evidence has been emerging that implicates epigenetics as a fundamental factor driving cellular rhythms and other physiological processes within organisms. The findings from these studies indicate the potential for episodic changes to be maintained across generations via an inheritable memory embodied at a subcellular level by genetic mechanisms that operate independently of traditional Mendelian laws. Our investigations have used a novel approach to uncover how this inherited epigenetic component can affect the circadian periods of individual clonal cell lines over time. To achieve our objectives, we developed modified culture growth conditions combined with single-cell RNA sequencing techniques and chromatin immunoprecipitation assays to observe advances in temporal regulation patterns. These results advance our understanding of heredity from a classical genetic perspective toward one guided by intricate transcriptional networks modulated through covalent histone modifications associated with rhythmic behaviour in non-genetically identical daughter cells derived from a common parent line. With greater insight into these pathways, more sophisticated systems engineering may become available for applications aimed towards improving health outcomes related to sleep dysregulation disorders such as Alzheimer’s Disease or Depression-Anxiety SyndromeObjective: The goal was to develop a pediatric airway stent for treating tracheobronchomalacia that could be used as an alternative to positive pressure ventilation. The design goals were for the stent to allow mucus flow and to resist migration inside the airways, while also enabling easy insertion and removal. Methods: A helical stent design, together with insertion and removal tools, is presented. A mechanics model of stent compression is derived to assist in selecting stent design parameters (pitch and wire diameter) that provide the desired amount of tracheal support, while introducing the minimal amount of foreign material into the airway. Worst-case airway area reduction with stent support is investigated experimentally using a pressurized tracheal phantom matched to porcine tracheal tissue properties. The stent design is then evaluated in a porcine in vivo experiment. Results: Phantom testing validated the mechanics model of stent compression. In vivo testing demonstrated that the stent was well tolerated by the animal. Since the helical design covers only a small portion of the epithelium, mucus transport through the stented region was minimally impeded. Furthermore, the screw-like stent resisted migration, while also providing for atraumatic removal through the use of an unscrewing motion during removal. Conclusion: The proposed stent design and tools represent a promising approach to prevent airway collapse in children with tracheobronchomalacia. Significance: The proposed technology overcomes the limitations of existing airway stents and may provide an alternative to maintaining children on a ventilator.Transmission of infection in the paediatric office is an issue of increasing concern. This document discusses routes of transmission of infection and the principles of current infection control measures. Prevention includes appropriate office design and administrative policies, triage, routine practices for the care of all patients (e.g., hand hygiene; use of gloves, masks, eye protection, and gowns for specific procedures; adequate cleaning, disinfection, and sterilization of surfaces and equipment, including toys; and aseptic technique for invasive procedures), and additional precautions for specific infections. Personnel should be adequately immunized, and those infected should follow work-restriction policies.STUDY DESIGN: Prospective case series. OBJECTIVE: Investigate the association of testosterone and thyroid-stimulating hormone (TSH) levels with depressive symptoms in women after spinal cord injury (SCI). SETTING: Community SCI clinic. METHODS: Twenty-seven participants were enrolled in this study. Total testosterone (Total T) and TSH levels as well as the Center for Epidemiological Studies Depression Scale (CES-D) survey and monthly sexual activity were obtained from only 20 participants. Pearson’s correlations were used to assess the relationship between age, time from injury, Total T level, TSH level, and CES-D total score. Follow-up analyses investigating the role of monthly sexual activity was also explored. RESULTS: Participants’ average age and time from injury was 44.4 ± 12.7 years old and 11.7 ± 8.89 years, respectively. Low Total T was observed in four participants and one of those participant’s presented with low TSH as well. Nine women were classified as “at risk for clinical depression” on the CES-D (total score >15). Pearson’s correlations revealed a significant association between time from injury and TSH (r = .536, p = .015), as well as CES-D total score (r = −.547, p = .013). Total T was associated with CES-D total score (ρ = −.541, p = .02). CONCLUSIONS: This study provides preliminary results on abnormal hormone levels and depressive symptoms in women after SCI. Twenty percent of this sample presented with low Total T, which was associated with increased depressive symptoms after accounting for time from injury. Further research is needed to investigate the impact of SCI on hormone function and mental health in women post SCI. SPONSORSHIP: Sally Rynne National Association of Women’s Health Quality Award 2002.\\n\\nThis study aims to analyze the recombinant patterns of Enterovirus 71 (EV71) circulating in Mainland China between 2009 and 2018. The research will utilize genome-wide nucleotide sequencing data collected from clinical samples during the study period, enabling a comprehensive analysis of both natural intertypic and intratypic recombinants. By mapping these genomes and comparing their phylogenetic relatedness with other EV71 strains worldwide, it is expected that this work can provide insights into the molecular epidemiology, evolution and spread of EV71 in recent years. This is important for understanding its caused diseases, such as HFMD and neurological complications which are frequently seen among Chinese children population; providing valuable epidemiological information that may be helpful for future public health strategies against EV71 infections.\\n\\nThis study examines the effects of robotic upper limb training in the subacute phase for four participants with cervical spinal cord injury. Through a single-subject design, increases were observed on several outcomes related to motor recovery and functional performance compared with baseline measures following two weeks of intermittent interventions. The results suggest that intense repetitive tasks, which can be achieved through robotic intervention techniques may facilitate recovery more effectively than other therapy modalities alone. Further research needs to be undertaken incorporating more persons and now exploring the long term benefits; ultimately allowing clinicians treating similar conditions insight into whether this type of sequence is enabled in clinical practice as an adjunct or sole treatment approach.Analysis of hospital admissions for 20 years suggests that there has been an increase in childhood asthma in Brisbane. The characteristic seasonal pattern of asthma with waves in autumn and spring is evident from the second year of age and continues into adult life. It has not been explained, although respiratory infections, allergens and cold changes probably contribute to it. Unlike adults, children shown an increase in asthma in February‐March, ascribed to infections spread at school. Maximal asthma is associated with a mean temperature of 20–21°C. This may be optimal for the production of allergens. Further viral studies of asthmatic attacks are desirable.\\n\\nThis article describes a systems pharmacology study of the anti-liver injury mechanism of citri reticulatae pericarpium, which is an ingredient contained in Traditional Chinese Medicine (TCM). The liver is known as one of the most important organs in terms of metabolism and excretion. Unbalanced lifestyle and environmental pollution can lead to serious liver damage or even chronic hepatic diseases. Therefore, herbal medicines are increasingly popular for their remarkable curative effects on treating various kinds of liver injuries due to their minimal side effects. In this study, we focus on Citri Reticulatae Pericarpium (CRP) by using a multi-tier approach that combines pharmacodynamic analysis with target identification and network biology evaluation for revealing its underlying mechanisms associated with its protection against hepatoxicity. This research considers key proteins related to drug metabolism as well as related signal transduction pathways from public databases by utilizing published literature, including obtaining bioinformatic results from global metabolomic profiling data based on NMR spectra screening methodologies combined with metaproteomic sequencing approaches. Meanwhile, CRP’s efficacy on additional disease models will be further evaluated by advances analytical procedures such as RNA landscape assay resulting in more accurate levels profiles integrated into functional prediction networks associated with hepatotoxic stress responses along the systemic space transition during test animal trials performed throughout this project.\\n\\nThis study explores the efficacy of employing monoclonal antibody to surfactant protein A (MAb-SPA) in reducing bacterial proliferation and improving lung function among ventilated rabbits exhibiting symptoms of pneumonia. In particular, we assess its effects on acute inflammatory responses as well as short-term follow up after infection. To this end, a randomized controlled trial was conducted using several groups: a control group receiving no interventi'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import trange, tqdm\n",
    "raw_text1 = \"\"\n",
    "for i in trange(len(ds1[\"train\"])):\n",
    "    raw_text1 += ds1[\"train\"][i][\"abstract\"]\n",
    "raw_text1[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T10:07:09.171866Z",
     "iopub.status.busy": "2025-05-13T10:07:09.171240Z",
     "iopub.status.idle": "2025-05-13T10:07:09.272312Z",
     "shell.execute_reply": "2025-05-13T10:07:09.271398Z",
     "shell.execute_reply.started": "2025-05-13T10:07:09.171838Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"raw_text1.txt\",\"w\") as file:\n",
    "    file.write(raw_text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"raw_text1.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    raw_text1 = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source', 'id', 'text'],\n",
       "        num_rows: 1392522\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds2 = load_dataset(\"artem9k/ai-text-detection-pile\",cache_dir=\"./dataset/\")\n",
    "ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12 Years a Slave: An Analysis of the Film Essay\\n\\nThe 2013 film 12 Years a Slave proved that slavery is a worldwide issue. Indeed, the film made $150 million outside the United States and $57 million in the U.S., with a production budget of $20 million (Sharf, 2020). The movie was based on the memoir Twelve Years a Slave by Solomon Northup (Ntim, 2020). It tells the story of a free African American man who was kidnapped and sold into slavery. Solomon spent twelve years away from his family, being traded from one master to another. Fortunately, the protagonist met a person who helped him deliver a message to his family and friends, who came and rescued him. This movie accurately illustrates discriminatory relationships between white slaveholders and black slaves that stemmed from the dysfunctional system in the country and prejudices in people’s mindsets at that time.\\n\\nThe two main ethnic groups presented in this film are White and African Americans, and the three social groups are affluent slaveholders, working for middle class, and enslaved people. The movie starts with the story of a free African American violinist Solomon Northup, living with his family in Saratoga, New York (McQueen, 2013). However, he was abducted by two white men, who tortured the man and sold him into slavery, changing his name to Plat. Before they met, Solomon and these two slave traders belonged to the same middle class. However, the fact that Northup was an African American made these individuals believe that they had the right to withdraw their freedom. The two masters that Solomon had were William Ford and Edwin Epps (McQueen, 2013). The former was kind and religious, while the latter was cruel and sadistic. Since the movie was based on a real story, it indicated that slaveholders had different characters, but all had the wrong perception of race.\\n\\nAlthough 12 Years a Slave is a film about slavery, the issues of collectivism and individualism are also raised. Specifically, the main character never identified himself as an enslaved man and continued claiming he was a free citizen (McQueen, 2013). However, his counterparts on the plant had a collective mindset, imprinted in them since childhood, that slavery is normal. These people helped each other because they belonged to the same group. Although Solomon tried to become a part of this community, his individual goal to return home was above the collective values.\\n\\nThe movie also showed prejudice, generalizations, stereotyping, and discrimination against black people. For instance, when Ford brings Solomon and Elisa to his plantation, his wife expresses her sadness that Elisa got separated from her children. However, she also stated that “something to eat and some rest” could help that woman forget her children (McQueen, 2013, 32:47-32:51). This scene demonstrated the common prejudice about slaves that they were not capable of the same feelings as white people. An example of generalization and stereotyping was how Tibeats, a carpenter, became hostile to Solomon when he showed his intelligence and gave Ford advice. In fact, Tibeats believed that Plat would never be more competent than any white individual because Plat was a “nigger” (McQueen, 2013, 36:35-36:37). Notably, before Northup became enslaved, he never experienced discrimination, but when the main character was sold into slavery, discrimination was the only attitude that he could observe.\\n\\nIn summary, 12 Years a Slave depicts the life of enslaved people and slave owners almost two centuries ago. The film narrates a free black man’s life from the moment when he enjoyed his family’s company in the state of New York to his abduction, enslavement, and eventual liberation. Overall, the movie raised such critical issues as discrimination, prejudice, stereotyping, and generalization that allowed slaveholders to maintain this societal structure for a long time.\\n\\nReferences\\n\\nMcQueen, S. (2013). 12 years a slave [Film]. New Regency Productions.\\n\\nNtim, Z. (2020). Steve McQueen says it took 11 years to create his new anthology “Small Axe” and reveals why producers almost pulled out of his Oscar-winning film “12 Years a Slave.” Insider.\\n\\nSharf, Z. (2020). Steve McQueen recalls producers rejecting “12 Years a Slave” over false beliefs about black films. Indie Wire.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds2[\"train\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c48145a468be45229c4c80da5aa00d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1392522 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m raw_text2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;28mlen\u001b[39m(ds2[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m])):\n\u001b[1;32m----> 4\u001b[0m     raw_text2 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mds2\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      5\u001b[0m raw_text2[:\u001b[38;5;241m500\u001b[39m]\n",
      "File \u001b[1;32me:\\Codes\\Coding_packages\\python310\\lib\\site-packages\\datasets\\arrow_dataset.py:2777\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2775\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[0;32m   2776\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2777\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Codes\\Coding_packages\\python310\\lib\\site-packages\\datasets\\arrow_dataset.py:2761\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[1;34m(self, key, **kwargs)\u001b[0m\n\u001b[0;32m   2759\u001b[0m format_kwargs \u001b[38;5;241m=\u001b[39m format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m   2760\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[1;32m-> 2761\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2762\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[0;32m   2763\u001b[0m     pa_subtable, key, formatter\u001b[38;5;241m=\u001b[39mformatter, format_columns\u001b[38;5;241m=\u001b[39mformat_columns, output_all_columns\u001b[38;5;241m=\u001b[39moutput_all_columns\n\u001b[0;32m   2764\u001b[0m )\n\u001b[0;32m   2765\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[1;32me:\\Codes\\Coding_packages\\python310\\lib\\site-packages\\datasets\\formatting\\formatting.py:610\u001b[0m, in \u001b[0;36mquery_table\u001b[1;34m(table, key, indices)\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;66;03m# Query the main table\u001b[39;00m\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 610\u001b[0m     pa_subtable \u001b[38;5;241m=\u001b[39m \u001b[43m_query_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    612\u001b[0m     pa_subtable \u001b[38;5;241m=\u001b[39m _query_table_with_indices_mapping(table, key, indices\u001b[38;5;241m=\u001b[39mindices)\n",
      "File \u001b[1;32me:\\Codes\\Coding_packages\\python310\\lib\\site-packages\\datasets\\formatting\\formatting.py:84\u001b[0m, in \u001b[0;36m_query_table\u001b[1;34m(table, key)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;124;03mQuery a pyarrow Table to extract the subtable that correspond to the given key.\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfast_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n\u001b[0;32m     86\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39mkey\u001b[38;5;241m.\u001b[39mindices(table\u001b[38;5;241m.\u001b[39mnum_rows))\n",
      "File \u001b[1;32me:\\Codes\\Coding_packages\\python310\\lib\\site-packages\\datasets\\table.py:151\u001b[0m, in \u001b[0;36mIndexedTableMixin.fast_slice\u001b[1;34m(self, offset, length)\u001b[0m\n\u001b[0;32m    149\u001b[0m     batches[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m batches[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mslice(\u001b[38;5;241m0\u001b[39m, offset \u001b[38;5;241m+\u001b[39m length \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offsets[j])\n\u001b[0;32m    150\u001b[0m     batches[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m batches[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mslice(offset \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offsets[i])\n\u001b[1;32m--> 151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_schema\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.auto import trange, tqdm\n",
    "raw_text2 = \"\"\n",
    "for i in trange(len(ds2[\"train\"])):\n",
    "    raw_text2 += ds2[\"train\"][i][\"text\"]\n",
    "raw_text2[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-13T09:53:50.646Z",
     "iopub.execute_input": "2025-05-13T04:43:10.638427Z",
     "iopub.status.busy": "2025-05-13T04:43:10.637991Z",
     "iopub.status.idle": "2025-05-13T04:43:10.662282Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31282705"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token = \"\") # you token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "925b54050445486698874aed4abaa33d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-cced8514c7ed782a.parquet:   0%|          | 0.00/41.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe230235817457ea9298eb01a1056c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/33000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question_id', 'model_a', 'model_b', 'winner', 'judge', 'conversation_a', 'conversation_b', 'turn', 'anony', 'language', 'tstamp', 'openai_moderation', 'toxic_chat_tag'],\n",
       "        num_rows: 33000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds3 = load_dataset(\"lmsys/chatbot_arena_conversations\")\n",
    "ds3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'What is the difference between OpenCL and CUDA?',\n",
       "  'role': 'user'},\n",
       " {'content': 'OpenCL and CUDA are both programming languages for parallel computing on GPUs, but they differ in several key ways:\\n\\n1.   Architecture: OpenCL is a general-purpose parallel computing language, while CUDA is designed specifically for GPU computing. OpenCL can run on various types of processors, including CPUs, GPUs, and FPGAs, whereas CUDA is only compatible with NVIDIA GPUs.\\n2.   Language: OpenCL is a C++ language, while CUDA is a C++ language with additional features for GPU computing. CUDA provides additional libraries and tools for handling graphics and other tasks unique to GPUs.\\n3.   Driver support: OpenCL has a wider range of device support, with support for not just NVIDIA GPUs but also other manufacturers like AMD, Intel, and ARM. CUDA is only available on NVIDIA GPUs, and requires the CUDA Toolkit to be installed on the host machine.\\n4.   Memory management: OpenCL provides a unified memory model, where all the data is stored in global memory, and the kernel can access any part of it. CUDA, on the other hand, uses a hierarchical memory architecture, where data is divided into host, device, and shared memory.\\n5.   Performance: OpenCL and CUDA have different strengths and weaknesses in terms of performance. OpenCL is generally faster for numerical computations, while CUDA is faster for graphics and other tasks that require fast memory access. CUDA also has better support for 64-bit floating-point operations, which are essential for some scientific applications.\\n\\nIn summary, OpenCL and CUDA are both programming languages for parallel computing on GPUs, but they differ in terms of architecture, language, driver support, memory management, and performance. OpenCL is a general-purpose language that can run on various types of processors, while CUDA is specific to NVIDIA GPUs and provides additional libraries and tools for handling graphics and other tasks unique to GPUs.',\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds3[\"train\"][0][\"conversation_b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the difference between OpenCL and CUDA?\n",
      "It is likely that they wanted to keep the guest list small and intimate. Maybe they felt that inviting you would make the event too crowded, or that they wanted to keep the event reserved for close friends and family. It could be that they wanted to avoid any potential drama or conflict that could arise from having too many people at the event. Whatever the reason, it’s important to remember that they made this decision out of love and respect for you, and that their decision doesn’t define your relationship.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(ds3[\"train\"][0][\"conversation_b\"])):\n",
    "    print(ds3[\"train\"][i][\"conversation_b\"][i][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122bd5f40ee04e86a4f28d99450eaca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'What is the difference between OpenCL and CUDA?OpenCL and CUDA are both programming languages for parallel computing on GPUs, but they differ in several key ways:\\n\\n1.   Architecture: OpenCL is a general-purpose parallel computing language, while CUDA is designed specifically for GPU computing. OpenCL can run on various types of processors, including CPUs, GPUs, and FPGAs, whereas CUDA is only compatible with NVIDIA GPUs.\\n2.   Language: OpenCL is a C++ language, while CUDA is a C++ language with '"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from tqdm.auto import trange, tqdm\n",
    "raw_text3 = \"\"\n",
    "for i in trange(len(ds3[\"train\"])):\n",
    "    for j in range(len(ds3[\"train\"][i][\"conversation_b\"])):\n",
    "        raw_text3 += ds3[\"train\"][i][\"conversation_b\"][j][\"content\"]\n",
    "raw_text3[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"raw_text3.txt\",\"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(raw_text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"raw_text3.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    raw_text3 = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09aa4212f773438db711d6663a962cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/2.82k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973a0f61f8ab43c883b2cfab8ac78a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "combined_dataset.json:   0%|          | 0.00/4.79M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "502abc94bf4e4f5bb9aa05a16963163d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3512 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Context', 'Response'],\n",
       "        num_rows: 3512\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds4 = load_dataset(\"Amod/mental_health_counseling_conversations\")\n",
    "ds4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Context': \"I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.\\n   I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it.\\n   How can I change my feeling of being worthless to everyone?\",\n",
       " 'Response': \"If everyone thinks you're worthless, then maybe you need to find new people to hang out with.Seriously, the social context in which a person lives is a big influence in self-esteem.Otherwise, you can go round and round trying to understand why you're not worthless, then go back to the same crowd and be knocked down again.There are many inspirational messages you can find in social media. \\xa0Maybe read some of the ones which state that no person is worthless, and that everyone has a good purpose to their life.Also, since our culture is so saturated with the belief that if someone doesn't feel good about themselves that this is somehow terrible.Bad feelings are part of living. \\xa0They are the motivation to remove ourselves from situations and relationships which do us more harm than good.Bad feelings do feel terrible. \\xa0 Your feeling of worthlessness may be good in the sense of motivating you to find out that you are much better than your feelings today.\"}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds4[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c37766d151b44a49e32c966eeb9a752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.\\n   I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it.\\n   How can I change my feeling of being worthless to everyone?If everyone thinks you're worthless, then maybe you need to find new people to hang out with.Seriously, the social context in which a person lives is a big influence in sel\""
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import trange, tqdm\n",
    "raw_text4 = \"\"\n",
    "for i in trange(len(ds4[\"train\"])):\n",
    "    raw_text4 += ds4[\"train\"][i][\"Context\"]\n",
    "    raw_text4 += ds4[\"train\"][i][\"Response\"]\n",
    "raw_text4[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"raw_text4.txt\",\"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(raw_text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"raw_text4.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    raw_text4 = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b432867897fb49859675f3f13f7a8e8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/535 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17935dfab01f4c45a28d0b62de443bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/52.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935e83a1cba446b1a19b3ed62320635b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/1.59M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a623174de74a52803826c959fa4fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/13.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deea8b8aa9f547e589b63b1d10ecf26e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e9ef6274da4697a1a7cde7552cfa71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb555fa95cf649ce8765f989619868f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['judgement', 'summary'],\n",
       "        num_rows: 3599\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['judgement', 'summary'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['judgement', 'summary'],\n",
       "        num_rows: 900\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds5 = load_dataset(\"Yashaswat/Indian-Legal-Text-ABS\")\n",
    "ds5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'judgement': \"Appeal No. 328 of 1962.\\nAppeal by special leave from the judgment and order dated September 4, 1961, of the Allahabad High Court in Civil Misc.\\nwrit No. 3469 of 1960.\\nK. L. Misra, Advocate General for the State of U. P., C. B. Agarwala, K. section Hajela and C. P. Lal, for the appellants.\\nsection P. Sinha and M. 1.\\nKhowaja, for respondent No. 1. 1962.\\nAugust 27.\\nThe Judgement of the Court was delivered by GAJENDRAGADKAR, J.\\nThis appeal by special leave arises out of a Writ Petition filed by the respondent Bagleshwar Prasad against the Board of High School and Intermediate Education, U. P., Allahabad, and its Secretary, appellants 1 & 2, and another.\\nBy his petition, the respondent challenged the validity of the order passed by appellant No. 1 on December 5, 1960, cancelling the respondent 's result at the High School Examination held in 1960.\\nIt appears that the respondent appeared for the said examination from the Nehru Intermediate College Centre, Bindki.\\nHe was declared to have passed the said examination in the 11 Division with distinction in article Thereafter, he joined Intermediate first year class in the Kulbaskar Ashram Agriculture College at Allahabad.\\nOn the 3rd September, 1960, he received a letter from the Principal, Adarsh Higher Secondary School, Kora Jahanabad, from where he had appeared for the High School examination, calling upon him to appear before a Sub Committee to answer the charge of having used unfair means in English, 769 Mathematics and Hindi papers.\\nAccordingly, he appeared before the said Sub Committee.\\nA charge was given to him and his explanation was obtained on the said charge.\\nThis charge was based on the fact that in Hindi 3rd paper set at the said examination, the respondent had given wrong answers to Question No. 4 in precisely the same firm in which the said answers had been given by a candidate whose Roll No. was 91733.\\nThe respondont 's Roll No. was 91731.\\nThe respondent was shown the identical wrong answers to the said Question which were found in the two papers, and he was asked to explaining about the said identity of the wrong answers.\\nHe admitted that the wrong answers appeared to be identical, but he denied that he had used any unfair means.\\nThe Sub Committee however, was not satisfied with the explanation and reported that both the respondent and the candidate whose Roll No. was 94733 had used unfair means.\\nAs a result of the report made by the Sub Committee, the first appellant passed an order cancelling the results of both the candidates.\\nBoth the said candidates disputed the validity of the said order, in the Allahabad High Court.\\nThe petition filed by the candidate whose Roll No. was 94733 was dismissed, but that of the respondent was allowed, and the impugned order passed by appellant No 1 cancelling, the result of the respondent in the High School examination for 1960, has been set aside.\\nIt is against this order that the appellants have come to this Court by special leave.\\nFrom the petition field by the the High Court (W. P. No. 3469 of 1960) it appears that he challenged the validity of the impugned order on several grounds.\\nThe principal contentions raised by the petitioner against the competence and the authority of appellant No.1 and against the regularity and fairness of the enquiry held, srose for decision before the High Court in the companion W. P. No. 3196 of 1960 also.\\nThe High 770 Court rejected the said contentions of law in that W. P. and for the reasons recorded in the judgment in that petition, the said contentions were rejected even in the present petition.\\nThus, the challenge to the validity of the order made on points of law was not sustained.\\nThe High Court then proceeded to examine the narrow ground of attack against the validity of the order which was made on the basis that the impugned order was not supported by any evidence at all.\\nIt appears from the judgment of the High Court that the High court was inclined to accept this argument and it has set aside the order on the ground that it is not supported by any evidence.\\nThe correctness of this finding is seriously disputed before us by the learned Advocate General who appears for the appellants.\\nIt is common ground that the proceed in taken against the respondent in respect of the unfair means alleged to have been adopted by him at the examination, are in the nature of quasi judicial proceedings, and as such, in a proper case, orders passed as a result of the said proceedings would be liable to be challenged under Art.226 of the Constitution.\\nIt is also common ground that the High Court would be justified in quashing the impugned order if it is satisfied that the said order is not based on any evidence at all.\\nAn order passed by a Tribunal holding a quasi judicial enquiry which is not supported by any evidence, is an order which is erroneous on the face of it and as such, is liable to be quashed by the High Court in exercise of its high prerogative jurisdiction to issue a writ under article 226.\\nIn the present case, the High Court has found that the conclusion of the enquiry Committee that the respondent had copied either from the answer book of the candidate bearing Roll No 947 3 or 771 from a common source, was not supported by any evidence In coming to this conclusion, the High Court has assumed that the charge against the respondent was that he had copied from the candidate bearing Roll No. 94733.\\nHaving made this assumption, the High Court has observed that there was no charge against the respondent that he connived in the act of copying by the other candidate ,from his answer book, and it has added that there is no evidence in proof of such connivance.\\nThe High Court has also stated that no evidence had been shown to justify the allegations that any outsider had helped the candidate, including the respondent.\\nThat, in brief, is the genesis of the final conclusion of the High Court.\\nIt appears that the High Court was in error in assuming that the only charge against the respondent was that he had copied from the paper of the candidate bearing Roll No. 94733 and this error is basically responsible for the other observations made by the High Court.\\nThe translation of the charge as it has been printed in the record before us, no doubt, seems to support the assumption made by the High Court in regard to the nature of the charge ' But the charge was framed in Hindi and it is common ground before us that the Hindi charge has not been properly translated from the record when it seems to show that what was alleged against the respondent was only that he had copied out from candidate bearing Roll No. 94733.\\nThe charge, in terms, was that having regard to the identity of the mistaken answers, the apprehension was that there had been copying, and that is very different from saying that the only charge was that the respondent had copied from the other candidate.\\nThis position is made very clear when we consider the explanation given by the respondent.\\nIn his explanation, the respondent bad 772 stated that he had not copied out from the answer , book of any candidate, nor had he allowed anyone to copy out from his answer book, so far as he could.\\nHe admitted that the mistaken answers in the two papers were identical and he pleaded 'that he could not say any thing as to why this happened.\\nHe was also asked whether he had got any help from outside and he gave an answer in the negative.\\nIt would thus be seen that at the enquiry, the charge against the respondent was, either that he copied from candidate bearing Roll No. 94733, or that he connived at the said candidate copying from his answer book, or that both of them had copied from a common source.\\nIn either case, 'it would amount to the adoption, of unfair means.\\nTherefore, in our opinion, the High Court was in error in assuming that the charge was very narrow and did not include the two other alternatives on which the adoption of unfair means was sought to be established.\\nThere is another circumstance which is relevant and significant and that has been ignored by the High Court in dealing with this petition: It appears that at the examination held at Bindki Centre, unfair means were adopted on a very large scale by a large number of students and the examination appears to have been conducted in an atmosphere which was not at all congenial to the enforcement of the discipline which has to be observed in conducting examinations.\\nIt appears that there are rivalries and party politics in the Municipal Board of Bindki that runs the institution at which this examination was held, and there are rivalries and party politics even amongst the members of the staff.\\nThe members of the Municipal Board and other influential people of the locality bring undue pressure on the Principal and the Invigilators to help their wards or the wards 773 of their friends and relatives in the Board 's Examination.\\nAs a result of this unhealthy atmosphere, the Centre at Bindki for High School examination had been abolished for some years, but on account of public pressure it was re started in 1960, and the result was very unfortunate.\\nIt also appears that on the day of English paper, while students were answering the paper in Room No. 3, an answer paper by some outsider was dropped into the room 15 minutes before the time to answer questions was over. ' This paper was thrown in room No. 3 from room No. 18.\\nIt was a typed paper giving answers to all the Questions.\\nThe Assistant teacher, Khajuha, who was one of the Invigilators, complained that the Parcha was typed in the office of the Superintendent of the Centre, but this allegation was denied.\\nIndeed, from the reports made by the invigilators and the findings made by the Enquiry Committee, it appears that the Invigilators themselves were so much frightened by the prevailing rowdyism and by pressure from influential people that they found themselves powerless to maintain discipline in the examination hall.\\nIt is, therefore, not surprising that some invigilators could not prevent copying and in fact, six of them had to be warned to be careful in future.\\nThe report of the enquiry committee also shows that the complaints which they were to investigate referred to copying on a large scale in several papers besides Hindi, and it is after examining all the complaints in the light of the evidence available to them that the Committee made its final report; and in that report, it held that the respondent and candidate bearing Roll No. 94733 were guilty of having used unfair means.\\n774 In dealing with the question as to whether the Committee was justified in coming to this conclusion against the respondent, it would not be reasonable to exclude from consideration the circumstances under which the whole enquiry came to be hold and the general background of the prevailing disturbed and riotous atmosphere in the Examination Hall during the days that the High School Examination was held at the Centre in 1960.\\nUnfortunately, the High Court has ignored this background altogether.\\nBefore the High Court, a statement was filed showing the seating arrangement in Room No. 10 where the respondent was sitting for writing his answers.\\nIt appears that he was No. 3 in the 3rd row, whereas the other candidate with Roll No. 94733 was No. 4 in the second row.\\nThe High Court was very much impressed by the fact that the respondent could not have looked back and copied from the answer.\\nbook of the other candidate, and the High Court did not think that there was any evidence to show that the other candidate could have copied from the respondents paper with his connivance.\\nWe have looked at the incorrect answers ourselves and we are not prepared to hold that the identical incorrect answers were given by the two candidates either by accident or by coincidence.\\nSome of the incorrect answers, and, particularly, the manner in which they have been given, clearly suggest that they were the result of either one candidate copying from the other, or both candidates copying from a common source.\\nThe significance of this fact has been completely missed by the High Court.\\nThe question before the Enquiry Committee had to be decided by it in the light of the nature of the incorrect answers themselves, and that is what the Enquiry Committee has done.\\nIt would, we think 775 be inappropriate in such a case to require direct evidence to show that the respondent could have looked back and copied from the answer written by the other candidate who was sitting behind him.\\nThere was still the alternative possibility that the candidate sitting behind may have copied from the respondent with his connivance.\\nIt is also not unlikely that the two candidates may have talked to each other.\\nThe atmosphere prevailing in the Examination Hall does not rule out this possibility.\\nThese are all matters which the Enquiry Committee had to consider, and the fact.\\nthat the Enquiry Committee did not write an elaborate report, does not mean that it did not consider all the relevant facts before it came to the conclusion that the respondent had used unfair ' means.\\nIn dealing with petitions of this type, it is necessary to bear in mind that educational institutions like the Universities or appellant No. 1 set up Enquiry Committees to deal with the problem posed by the adoption of unfair means by candidates, and normally it is within the jurisdiction of such domestic Tribunals to decide all relevant questions in the light of the evidence adduced before them.\\nIn the matter of the adoption of unfair means, direct evidence may sometimes be available, but cases may arise where direct evidence is not available and the question will have to be considered in the light of probabilities and circumstantial evidence.\\nThis problem which educational institutions have to face from time to time is a serious problem and unless there is justification to do so, courts should be slow to interfere with the decisions of domestic Tribunals appointed by educational bodies like the Universities.\\nIn dealing with the validity of the impugned orders passed by Universities under article 226, the High Court is not sitting in appeal over the decision in question; its jurisdiction is limited and though, 776 it is true that if the impugned order is not supported by any evidence, at all, the High Court would be justified to quash that order.\\nBut the conclusion that the impugned order is not supported by any evidence must be reached after considering the question as to whether probabilities and circumstantial evidence do not justify the said conclusion.\\nEnquiries held by domestic Tribunals in such cases must, no doubt, be fair and students against whom charges are framed must be given adequate opportunities to defend themselves and in holding such enquiries, the Tribunal, must scrupulously follow rules of natural justice; but it would, we think, not be reasonable to import into these enquiries all considerations which govern criminal trials in ordinary courts of law.\\nIn the present case, no animus is suggested and no malafides have been pleaded.\\nThe enquiry has been fair and the respondent has had an opportunity of making his defence.\\nThat being so, we think the High Court was not justified in interfering with the order passed against the respondent.\\nWe ought, however, to add that though we are inclined to accept the argument raised by the learned Advocate General against the decision of the High Court, we do not propose to make any consequential order is favour of the appellants, because the learned Advocate General has fairly conceded that he does not want any such order in the present appeal.\\nIt appears that the respondent has, in June, 1962, passed his Intermediate Examination and it has been fairly conceded that there is no intention to disturb his career under the present circumstances.\\nThe learned Advocate General wanted a decision from us in this appeal because he apprehended that the reasoning adopted by the High Court in setting aside the order passed against the respondent may be construed to mean that 777 under article 226, the High Court can examine the merits of the order passed by appellant No. 1 in such cases.\\nThe result is, though we agree with the appellants that the order passed by the High Court was not justified, we refrain from setting it aside for the reasons just explained.\\nThere would be no order as to costs.\\n\",\n",
       " 'summary': \"The appellant Board cancelled the declaration of the result of the respondent in the High School Certificate Examination held in 1960 accepting the findings of the subcommittee appointed by it to enquire into the charges made against the respondent and another candidate of having used unfair means in answering the English, Mathematics and Hindi papers.\\nThe charges were based upon the fact that in the Hindi 3rd paper set at the said examination, the respondent gave wrong answers to Question No. 4 in precisely the same form in which the answers had been given by the candidate whose Roll number was consecutive with that of the respondent.\\nThe High Court interpreting the charge as confined to that the respondent had copied either from the answer book of the candidate bearing the consecutive Roll Number or from a common source held that the findings of the enquiry committee were based on no evidence and quashed the cancellation of the result.\\nOn appeal by special leave.\\nHeld, that in the circumstances of the case, the identity of the wrong answers given by the respondent with that of the other candidate bearing the consecutive Roll Number rendered the charge of the respondent having employed unfair means highly probable and that the findings of the enquiry committee based upon such probabilities and circumstantial evidence could not be said to be based on no evidence as in such matters direct evidence quite often cannot be available.\\nHeld, further, that in dealing with cases like those of educational institutions dealing with matters of discipline like employing unfair means, the problem faced by the educational institutions should be appreciated by the 'High Court and so long as the enquiry held is fair and affords the candidate an opportunity to defend himself, the matter should 768 not be examined with the same strictness as applicable to criminal trials in the ordinary courts of law.\\n\"}"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds5[\"validation\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc4220295a04803bedb3fc2d6b458c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3599 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "624b434659a5410ca899177e28244250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28f3961e97484fffa833544c9928825f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"Appeal No. 623 of 1975.\\nFrom the Judgment and Order dated 25 6 74 of the Karna taka 'High Court in Civil Revision No 1981/73.\\nS.S. JavaIi and B.P. Singh, for the Appellants.\\nS.V. Gupte and K.N. Bhatt, for the Respondent.\\nThe Judgment of the Court was delivered by RAY, C.J.\\nThis appeal by special leave is from the judgment .dated 25 June, 1974 of the Karnataka High Court.\\nThe principal question in this appeal whether section 107 of the Karnataka Land Reforms Act, 1961 applies to the land in suit \""
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import trange, tqdm\n",
    "raw_text5 = \"\"\n",
    "for i in trange(len(ds5[\"train\"])):\n",
    "    raw_text5 += ds5[\"train\"][i][\"judgement\"]\n",
    "    raw_text5 += ds5[\"train\"][i][\"summary\"]\n",
    "\n",
    "for i in trange(len(ds5[\"test\"])):\n",
    "    raw_text5 += ds5[\"test\"][i][\"judgement\"]\n",
    "    raw_text5 += ds5[\"test\"][i][\"summary\"]\n",
    "\n",
    "for i in trange(len(ds5[\"validation\"])):\n",
    "    raw_text5 += ds5[\"validation\"][i][\"judgement\"]\n",
    "    raw_text5 += ds5[\"validation\"][i][\"summary\"]\n",
    "\n",
    "raw_text5[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"raw_text5.txt\",\"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(raw_text5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"raw_text5.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    raw_text5 = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-13T04:43:10.665858Z",
     "iopub.status.busy": "2025-05-13T04:43:10.665542Z",
     "iopub.status.idle": "2025-05-13T04:43:10.820526Z",
     "shell.execute_reply": "2025-05-13T04:43:10.818049Z",
     "shell.execute_reply.started": "2025-05-13T04:43:10.665831Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import collections\n",
    "from tqdm.auto import tqdm,trange\n",
    "import pickle\n",
    "import json\n",
    "import os \n",
    "\n",
    "class BPETOKENIZER():\n",
    "  def __init__(self):\n",
    "    self.merges = {}\n",
    "    self.vocab = {  \n",
    "                    \"<unk>\":0,\"<sos>\":1,\"<eos>\":2,\"<pad>\":3,\n",
    "                    'a': 4, 'b': 5, 'c': 6, 'd': 7, 'e': 8, 'f': 9, 'g': 10, 'h': 11, 'i': 12, 'j': 13,\n",
    "                    'k': 14, 'l': 15, 'm': 16, 'n': 17, 'o': 18, 'p': 19, 'q': 20, 'r': 21, 's': 22,\n",
    "                    't': 23, 'u': 24, 'v': 25, 'w': 26, 'x': 27, 'y': 28, 'z': 29,\n",
    "\n",
    "                    'A': 30, 'B': 31, 'C': 32, 'D': 33, 'E': 34, 'F': 35, 'G': 36, 'H': 37, 'I': 38, 'J': 39,\n",
    "                    'K': 40, 'L': 41, 'M': 42, 'N': 43, 'O': 44, 'P': 45, 'Q': 46, 'R': 47, 'S': 48,\n",
    "                    'T': 49, 'U': 50, 'V': 51, 'W': 52, 'X': 53, 'Y': 54, 'Z': 55,\n",
    "\n",
    "                    '0': 56, '1': 57, '2': 58, '3': 59, '4': 60, '5': 61, '6': 62, '7': 63, '8': 64, '9': 65,\n",
    "\n",
    "                    '!': 66, '@': 67, '#': 68, '$': 69, '%': 70, '^': 71, '&': 72, '*': 73, '(': 74, ')': 75,\n",
    "                    '-': 76, '_': 77, '=': 78, '+': 79, '[': 80, ']': 81, '{': 82, '}': 83, '\\\\': 84, '|': 85,\n",
    "                    ';': 86, ':': 87, \"'\": 88, '\"': 89, '<': 90, '>': 91, ',': 92, '.': 93, '?': 94, '/': 95,\n",
    "                    '`': 96, '~': 97, ' ': 98\n",
    "    }\n",
    "    self.pattern = r\"\\p{L}+\\d*|[^\\p{L}\\s]|[\\n\\r]|[\\p{Emoji}]\"\n",
    "\n",
    "  def split_chars(self,raw_text):\n",
    "    vocab = collections.defaultdict(int)\n",
    "    text = re.findall(self.pattern,raw_text)\n",
    "    text = [\" \".join(word) for word in text]\n",
    "    for word in text:\n",
    "      vocab[word] += 1\n",
    "    return vocab,text\n",
    "\n",
    "  def pair_maker(self,vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "      symbols = word.split()\n",
    "      for i in range(len(symbols)-1):\n",
    "        pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "  def pair_merge(self, pair, v_in):\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    return {p.sub(''.join(pair), word): freq for word, freq in v_in.items()}\n",
    "\n",
    "  def train(self,raw_text:str,iteration:int,save_every: int = 1,merges_path=\"merges.json\",vocab_path=\"vocab.json\"):\n",
    "    vocab,text = self.split_chars(raw_text)\n",
    "    for i in trange(iteration):\n",
    "      pairs = self.pair_maker(vocab)\n",
    "      if not pairs:\n",
    "        print(\"No more pairs to merge\")\n",
    "        break\n",
    "      best_pair = max(pairs,key = pairs.get)\n",
    "      if pairs[best_pair] <= 10:\n",
    "         print(\"Pairs frequency is less than 10 stopping the Training...\")\n",
    "      else:   \n",
    "        if \"\".join(best_pair) not in self.merges:\n",
    "            # print(f'new pair: {\"\".join(best_pair)} with freq: {pairs[best_pair]}')\n",
    "            vocab = self.pair_merge(best_pair,vocab)\n",
    "            self.merges[\"\".join(best_pair)] = {\"pair\" : best_pair,\"pair_freq_in_data\" : pairs[best_pair]}\n",
    "        else:\n",
    "            if self.merges[\"\".join(best_pair)][\"pair_freq_in_data\"] >= pairs[best_pair]:\n",
    "                # print(f'pair already there {\"\".join(best_pair)}')\n",
    "                pass\n",
    "            else:\n",
    "                # print(f'new pair: {\"\".join(best_pair)} with freq: {pairs[best_pair]}')\n",
    "                self.merges[\"\".join(best_pair)] = {\"pair\" : best_pair,\"pair_freq_in_data\" : pairs[best_pair]}\n",
    "                vocab = self.pair_merge(best_pair,vocab)\n",
    "\n",
    "        if save_every and (i + 1) % save_every == 0:\n",
    "          self.vocab_maker()\n",
    "          self._safe_save(merges_path,vocab_path,i)\n",
    "                \n",
    "    final_vocab = self.vocab_maker()\n",
    "    self._safe_save(merges_path,vocab_path,i)\n",
    "    return self.merges,final_vocab\n",
    "  \n",
    "  def _safe_save(self, merges_path,vocab_path,i):\n",
    "    merges_serializable = {\n",
    "        k: {\n",
    "            \"pair\": list(v[\"pair\"]),\n",
    "            \"pair_freq_in_data\": v[\"pair_freq_in_data\"]\n",
    "        } for k, v in self.merges.items()\n",
    "    }\n",
    "\n",
    "    with open(merges_path + \".tmp\", \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(merges_serializable, file, indent=4, ensure_ascii=False)\n",
    "    os.replace(merges_path + \".tmp\", merges_path)\n",
    "\n",
    "    with open(vocab_path + \".tmp\", \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(self.vocab, file, indent=4, ensure_ascii=False)\n",
    "    os.replace(vocab_path + \".tmp\", vocab_path)\n",
    "\n",
    "    print(f\"Checkpoint saved at merges: {merges_path} and vocab: {vocab_path} at iteration: {i}\\nlength of merges: {len(self.merges)} length of vocab: {len(self.vocab)}\")\n",
    "\n",
    "  def vocab_maker(self):\n",
    "    for idx,(pair,pair_info) in enumerate(self.merges.items()):\n",
    "        self.vocab[pair] = idx+99\n",
    "    return self.vocab\n",
    "\n",
    "  def encode(self, raw_text:list):\n",
    "      if isinstance(raw_text,list):\n",
    "         pass\n",
    "      else:\n",
    "         raw_text = [raw_text]\n",
    "      final_output = []\n",
    "      for idx in raw_text:\n",
    "        idx = idx.replace(\" \",\"·\")\n",
    "        vocab, text = self.split_chars(idx)\n",
    "        tokens = []\n",
    "        tokens.append(\"<sos>\")\n",
    "        tokens.append(\" \")\n",
    "\n",
    "        for word in text:\n",
    "          word = word.split()\n",
    "          while len(word) > 1:\n",
    "              pairs = [(word[i], word[i + 1]) for i in range(len(word) - 1)]\n",
    "              pair_found = False\n",
    "\n",
    "              for pair in pairs:\n",
    "                  if \"\".join(pair) in self.merges:\n",
    "                      new_token = \"\".join(pair)\n",
    "                      index = pairs.index(pair)\n",
    "                      word = word[:index] + [new_token] + word[index + 2:]\n",
    "                      pair_found = True\n",
    "                      break\n",
    "                  \n",
    "\n",
    "              if not pair_found:\n",
    "                  break\n",
    "\n",
    "          tokens.extend(word)\n",
    "        tokens.append(\" \")\n",
    "        tokens.append(\"<eos>\")\n",
    "\n",
    "        for token in tokens:\n",
    "          if token == \"·\":\n",
    "              index = tokens.index(token)\n",
    "              tokens[index] = \" \"\n",
    "        final_output.append(tokens)\n",
    "      \n",
    "      def stoi(input:list,vocab:dict):\n",
    "        output = []\n",
    "        for sentence in input:\n",
    "          s =[]\n",
    "          # s.append(vocab[\"<sos>\"])\n",
    "          for word in sentence:\n",
    "            if word in vocab:\n",
    "              s.append(vocab[word])\n",
    "            else:\n",
    "              s.append(vocab[\"<unk>\"])\n",
    "          # s.append(vocab[\"<eos>\"])\n",
    "          output.append(s)\n",
    "        return output\n",
    "      stoi_output = stoi(final_output,self.vocab)\n",
    "      return final_output,stoi_output\n",
    "  \n",
    "  def decode(self,input:list):\n",
    "    if isinstance(input,list):\n",
    "      pass\n",
    "      stoi_vocab = self.vocab_maker()\n",
    "      itos_vocab = {}\n",
    "      for k,v in stoi_vocab.items():\n",
    "        itos_vocab[v] = k\n",
    "      \n",
    "      def itos(input:list,vocab:dict):\n",
    "        output = []\n",
    "        for sentence in input:\n",
    "          s = []\n",
    "          for word in sentence:\n",
    "            if word in vocab:\n",
    "              s.append(vocab[word])\n",
    "            else:\n",
    "              s.append(vocab[0])\n",
    "          output.append(\"\".join(s))\n",
    "        return output\n",
    "      \n",
    "      itos_output = itos(input,itos_vocab)\n",
    "    else:\n",
    "       print(\"Input Should be a list\")\n",
    "    return itos_output\n",
    "      \n",
    "  def save(self, merges=\"merges.json\",vocab=\"vocab.json\"): \n",
    "      merges_serializable = {\n",
    "          k: {\n",
    "              \"pair\": list(v[\"pair\"]),\n",
    "              \"pair_freq_in_data\": v[\"pair_freq_in_data\"]\n",
    "          } for k, v in self.merges.items()\n",
    "      }\n",
    "    \n",
    "      with open(merges, \"w\", encoding=\"utf-8\") as file:\n",
    "          json.dump(merges_serializable, file, indent=4, ensure_ascii=False)\n",
    "    \n",
    "      with open(vocab, \"w\", encoding=\"utf-8\") as file:\n",
    "          json.dump(self.vocab, file, indent=4, ensure_ascii=False)\n",
    "    \n",
    "      print(f\"Saved merges to {merges}\")\n",
    "      print(f\"Saved vocab to {vocab}\")\n",
    "\n",
    "  def load(self, merges=\"merges.json\",vocab=\"vocab.json\"):\n",
    "      with open(merges, \"r\", encoding=\"utf-8\") as file:\n",
    "          merges_data = json.load(file)\n",
    "          \n",
    "      self.merges = {\n",
    "          k: {\n",
    "              \"pair\": tuple(v[\"pair\"]),\n",
    "              \"pair_freq_in_data\": v[\"pair_freq_in_data\"]\n",
    "          } for k, v in merges_data.items()\n",
    "      }\n",
    "      with open(vocab, \"r\", encoding=\"utf-8\") as file:\n",
    "          self.vocab = json.load(file)\n",
    "      \n",
    "      print(f\"Loaded merges from {merges}\")\n",
    "      print(f\"Loaded vocab from {vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:43:10.824777Z",
     "iopub.status.busy": "2025-05-13T04:43:10.823853Z",
     "iopub.status.idle": "2025-05-13T04:43:10.849595Z",
     "shell.execute_reply": "2025-05-13T04:43:10.839207Z",
     "shell.execute_reply.started": "2025-05-13T04:43:10.824738Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "bpe = BPETOKENIZER()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:43:10.866139Z",
     "iopub.status.busy": "2025-05-13T04:43:10.865742Z",
     "iopub.status.idle": "2025-05-13T04:43:11.056500Z",
     "shell.execute_reply": "2025-05-13T04:43:11.054265Z",
     "shell.execute_reply.started": "2025-05-13T04:43:10.866087Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded merges from merges.json\n",
      "Loaded vocab from vocab.json\n"
     ]
    }
   ],
   "source": [
    "bpe.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:43:11.075780Z",
     "iopub.status.busy": "2025-05-13T04:43:11.075275Z",
     "iopub.status.idle": "2025-05-13T04:43:11.094108Z",
     "shell.execute_reply": "2025-05-13T04:43:11.087096Z",
     "shell.execute_reply.started": "2025-05-13T04:43:11.075745Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85106, 85205)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bpe.merges),len(bpe.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<sos>', ' ', 'hello', ' ', 'how', ' ', 'are', ' ', 'you', ' ', '<eos>']]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens,stoi_output = bpe.encode(\"hello how are you\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 98, 3275, 98, 425, 98, 148, 98, 136, 98, 2]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> hello how are you <eos>']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe.decode(stoi_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-13T09:53:50.647Z",
     "iopub.execute_input": "2025-05-13T04:43:11.101297Z",
     "iopub.status.busy": "2025-05-13T04:43:11.100852Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc52bbf4efd4d14b499984ede605ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 99\n",
      "length of merges: 85102 length of vocab: 85201\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 199\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 299\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 399\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 499\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 599\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 699\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 799\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 899\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 999\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 1099\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 1199\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 1299\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 1399\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 1499\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 1599\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 1699\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 1799\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 1899\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 1999\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 2099\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 2199\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 2299\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 2399\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 2499\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 2599\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 2699\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 2799\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 2899\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 2999\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 3099\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 3199\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 3299\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 3399\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 3499\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 3599\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 3699\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 3799\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 3899\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 3999\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 4099\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 4199\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 4299\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 4399\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 4499\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 4599\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 4699\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 4799\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 4899\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 4999\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 5099\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 5199\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 5299\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 5399\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 5499\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 5599\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 5699\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 5799\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 5899\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 5999\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 6099\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 6199\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 6299\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 6399\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 6499\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 6599\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 6699\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 6799\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 6899\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 6999\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 7099\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 7199\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 7299\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 7399\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 7499\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 7599\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 7699\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 7799\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 7899\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 7999\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 8099\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 8199\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 8299\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 8399\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 8499\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 8599\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 8699\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 8799\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 8899\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 8999\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 9099\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 9199\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 9299\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 9399\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 9499\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 9599\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 9699\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 9799\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 9899\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 9999\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 10099\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 10199\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 10299\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 10399\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 10499\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 10599\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 10699\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 10799\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 10899\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 10999\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 11099\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 11199\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 11299\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 11399\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 11499\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 11599\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 11699\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 11799\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 11899\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 11999\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 12099\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 12199\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 12299\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 12399\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 12499\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 12599\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 12699\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 12799\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 12899\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 12999\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 13099\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 13199\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 13299\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 13399\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 13499\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 13599\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 13699\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 13799\n",
      "length of merges: 85106 length of vocab: 85205\n",
      "Checkpoint saved at merges: merges.json and vocab: vocab.json at iteration: 13899\n",
      "length of merges: 85106 length of vocab: 85205\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[313], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m merges,vocab \u001b[38;5;241m=\u001b[39m \u001b[43mbpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_text5\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10000000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[309], line 54\u001b[0m, in \u001b[0;36mBPETOKENIZER.train\u001b[1;34m(self, raw_text, iteration, save_every, merges_path, vocab_path)\u001b[0m\n\u001b[0;32m     52\u001b[0m vocab,text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_chars(raw_text)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m trange(iteration):\n\u001b[1;32m---> 54\u001b[0m   pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpair_maker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pairs:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo more pairs to merge\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[309], line 43\u001b[0m, in \u001b[0;36mBPETOKENIZER.pair_maker\u001b[1;34m(self, vocab)\u001b[0m\n\u001b[0;32m     41\u001b[0m   symbols \u001b[38;5;241m=\u001b[39m word\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m     42\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(symbols)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 43\u001b[0m     pairs[symbols[i],symbols[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m freq\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pairs\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "merges,vocab = bpe.train(raw_text5,10000000,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85200, 85299)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bpe.merges),len(bpe.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-13T09:53:50.647Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merges to merges.json\n",
      "Saved vocab to vocab.json\n"
     ]
    }
   ],
   "source": [
    "bpe.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-12T17:46:41.734Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "len(merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Appe',\n",
       "  'al',\n",
       "  ' ',\n",
       "  'No',\n",
       "  '.',\n",
       "  ' ',\n",
       "  '6',\n",
       "  '2',\n",
       "  '3',\n",
       "  ' ',\n",
       "  'of',\n",
       "  ' ',\n",
       "  '1',\n",
       "  '9',\n",
       "  '7',\n",
       "  '5',\n",
       "  '.',\n",
       "  'From',\n",
       "  ' ',\n",
       "  'the',\n",
       "  ' ',\n",
       "  'Judgment',\n",
       "  ' ',\n",
       "  'and',\n",
       "  ' ',\n",
       "  'Order',\n",
       "  ' ',\n",
       "  'dated',\n",
       "  ' ',\n",
       "  '2',\n",
       "  '5',\n",
       "  ' ',\n",
       "  '6',\n",
       "  ' ',\n",
       "  '7',\n",
       "  '4',\n",
       "  ' ',\n",
       "  'of',\n",
       "  ' ',\n",
       "  'the',\n",
       "  ' ',\n",
       "  'Kar',\n",
       "  'na',\n",
       "  ' ',\n",
       "  'tak',\n",
       "  'a',\n",
       "  ' ',\n",
       "  \"'\",\n",
       "  'High',\n",
       "  ' ',\n",
       "  'Court',\n",
       "  ' ',\n",
       "  'in',\n",
       "  ' ',\n",
       "  'Civil',\n",
       "  ' ',\n",
       "  'Revision',\n",
       "  ' ',\n",
       "  'No',\n",
       "  ' ',\n",
       "  '1',\n",
       "  '9',\n",
       "  '8',\n",
       "  '1',\n",
       "  '/',\n",
       "  '7',\n",
       "  '3',\n",
       "  '.',\n",
       "  'S',\n",
       "  '.',\n",
       "  'S',\n",
       "  '.',\n",
       "  ' ',\n",
       "  'Java',\n",
       "  'I',\n",
       "  'i',\n",
       "  ' ',\n",
       "  'and',\n",
       "  ' ',\n",
       "  'B',\n",
       "  '.',\n",
       "  'P',\n",
       "  '.',\n",
       "  ' ',\n",
       "  'Singh',\n",
       "  ',',\n",
       "  ' ',\n",
       "  'for',\n",
       "  ' ',\n",
       "  'the',\n",
       "  ' ',\n",
       "  'Appel',\n",
       "  'lan',\n",
       "  'ts',\n",
       "  '.',\n",
       "  'S',\n",
       "  '.',\n",
       "  'V',\n",
       "  '.',\n",
       "  ' ',\n",
       "  'Gu',\n",
       "  'pt',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'and',\n",
       "  ' ',\n",
       "  'K',\n",
       "  '.',\n",
       "  'N',\n",
       "  '.',\n",
       "  ' ',\n",
       "  'Bhat',\n",
       "  't',\n",
       "  ',',\n",
       "  ' ',\n",
       "  'for',\n",
       "  ' ',\n",
       "  'the',\n",
       "  ' ',\n",
       "  'Respond',\n",
       "  'ent',\n",
       "  '.',\n",
       "  'The',\n",
       "  ' ',\n",
       "  'Judgment',\n",
       "  ' ',\n",
       "  'of',\n",
       "  ' ',\n",
       "  'the',\n",
       "  ' ',\n",
       "  'Court',\n",
       "  ' ',\n",
       "  'was',\n",
       "  ' ',\n",
       "  'delivered',\n",
       "  ' ',\n",
       "  'by',\n",
       "  ' ',\n",
       "  'RA',\n",
       "  'Y',\n",
       "  ',',\n",
       "  ' ',\n",
       "  'C',\n",
       "  '.',\n",
       "  'J',\n",
       "  '.',\n",
       "  'This',\n",
       "  ' ',\n",
       "  'appeal',\n",
       "  ' ',\n",
       "  'by',\n",
       "  ' ',\n",
       "  'special',\n",
       "  ' ',\n",
       "  'leave',\n",
       "  ' ',\n",
       "  'is',\n",
       "  ' ',\n",
       "  'from',\n",
       "  ' ',\n",
       "  'the',\n",
       "  ' ',\n",
       "  'judgment',\n",
       "  ' ',\n",
       "  '.',\n",
       "  'dated',\n",
       "  ' ',\n",
       "  '2',\n",
       "  '5',\n",
       "  ' ',\n",
       "  'June',\n",
       "  ',',\n",
       "  ' ',\n",
       "  '1',\n",
       "  '9',\n",
       "  '7',\n",
       "  '4',\n",
       "  ' ',\n",
       "  'of',\n",
       "  ' ',\n",
       "  'the',\n",
       "  ' ',\n",
       "  'Kar',\n",
       "  'nat',\n",
       "  'aka',\n",
       "  ' ',\n",
       "  'High',\n",
       "  ' ',\n",
       "  'Court',\n",
       "  '.']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens,stoi_output = bpe.encode(\"\"\"Appeal No. 623 of 1975.\n",
    "From the Judgment and Order dated 25 6 74 of the Karna taka 'High Court in Civil Revision No 1981/73.\n",
    "S.S. JavaIi and B.P. Singh, for the Appellants.\n",
    "S.V. Gupte and K.N. Bhatt, for the Respondent.\n",
    "The Judgment of the Court was delivered by RAY, C.J.\n",
    "This appeal by special leave is from the judgment .dated 25 June, 1974 of the Karnataka High Court.\"\"\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-12T17:46:41.733Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1,\n",
       "  27645,\n",
       "  112,\n",
       "  98,\n",
       "  1044,\n",
       "  93,\n",
       "  98,\n",
       "  62,\n",
       "  58,\n",
       "  59,\n",
       "  98,\n",
       "  120,\n",
       "  98,\n",
       "  57,\n",
       "  65,\n",
       "  63,\n",
       "  61,\n",
       "  93,\n",
       "  2859,\n",
       "  98,\n",
       "  105,\n",
       "  98,\n",
       "  53619,\n",
       "  98,\n",
       "  114,\n",
       "  98,\n",
       "  7997,\n",
       "  98,\n",
       "  16806,\n",
       "  98,\n",
       "  58,\n",
       "  61,\n",
       "  98,\n",
       "  62,\n",
       "  98,\n",
       "  63,\n",
       "  60,\n",
       "  98,\n",
       "  120,\n",
       "  98,\n",
       "  105,\n",
       "  98,\n",
       "  6667,\n",
       "  1038,\n",
       "  98,\n",
       "  10941,\n",
       "  4,\n",
       "  98,\n",
       "  88,\n",
       "  3552,\n",
       "  98,\n",
       "  11154,\n",
       "  98,\n",
       "  100,\n",
       "  98,\n",
       "  7840,\n",
       "  98,\n",
       "  76552,\n",
       "  98,\n",
       "  1044,\n",
       "  98,\n",
       "  57,\n",
       "  65,\n",
       "  64,\n",
       "  57,\n",
       "  95,\n",
       "  63,\n",
       "  59,\n",
       "  93,\n",
       "  48,\n",
       "  93,\n",
       "  48,\n",
       "  93,\n",
       "  98,\n",
       "  2001,\n",
       "  38,\n",
       "  12,\n",
       "  98,\n",
       "  114,\n",
       "  98,\n",
       "  31,\n",
       "  93,\n",
       "  45,\n",
       "  93,\n",
       "  98,\n",
       "  22474,\n",
       "  92,\n",
       "  98,\n",
       "  135,\n",
       "  98,\n",
       "  105,\n",
       "  98,\n",
       "  36331,\n",
       "  7403,\n",
       "  326,\n",
       "  93,\n",
       "  48,\n",
       "  93,\n",
       "  51,\n",
       "  93,\n",
       "  98,\n",
       "  2641,\n",
       "  382,\n",
       "  8,\n",
       "  98,\n",
       "  114,\n",
       "  98,\n",
       "  40,\n",
       "  93,\n",
       "  43,\n",
       "  93,\n",
       "  98,\n",
       "  66348,\n",
       "  23,\n",
       "  92,\n",
       "  98,\n",
       "  135,\n",
       "  98,\n",
       "  105,\n",
       "  98,\n",
       "  14348,\n",
       "  128,\n",
       "  93,\n",
       "  169,\n",
       "  98,\n",
       "  53619,\n",
       "  98,\n",
       "  120,\n",
       "  98,\n",
       "  105,\n",
       "  98,\n",
       "  11154,\n",
       "  98,\n",
       "  276,\n",
       "  98,\n",
       "  10596,\n",
       "  98,\n",
       "  225,\n",
       "  98,\n",
       "  6248,\n",
       "  54,\n",
       "  92,\n",
       "  98,\n",
       "  32,\n",
       "  93,\n",
       "  39,\n",
       "  93,\n",
       "  270,\n",
       "  98,\n",
       "  10468,\n",
       "  98,\n",
       "  225,\n",
       "  98,\n",
       "  1803,\n",
       "  98,\n",
       "  3132,\n",
       "  98,\n",
       "  110,\n",
       "  98,\n",
       "  259,\n",
       "  98,\n",
       "  105,\n",
       "  98,\n",
       "  11326,\n",
       "  98,\n",
       "  93,\n",
       "  16806,\n",
       "  98,\n",
       "  58,\n",
       "  61,\n",
       "  98,\n",
       "  5275,\n",
       "  92,\n",
       "  98,\n",
       "  57,\n",
       "  65,\n",
       "  63,\n",
       "  60,\n",
       "  98,\n",
       "  120,\n",
       "  98,\n",
       "  105,\n",
       "  98,\n",
       "  6667,\n",
       "  1219,\n",
       "  5997,\n",
       "  98,\n",
       "  3552,\n",
       "  98,\n",
       "  11154,\n",
       "  93,\n",
       "  2]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-12T17:46:41.734Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T11:31:32.989378Z",
     "iopub.status.busy": "2025-05-12T11:31:32.988809Z",
     "iopub.status.idle": "2025-05-12T11:31:33.050826Z",
     "shell.execute_reply": "2025-05-12T11:31:33.049970Z",
     "shell.execute_reply.started": "2025-05-12T11:31:32.989347Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T10:04:29.129136Z",
     "iopub.status.busy": "2025-05-12T10:04:29.128771Z",
     "iopub.status.idle": "2025-05-12T10:04:29.149777Z",
     "shell.execute_reply": "2025-05-12T10:04:29.148918Z",
     "shell.execute_reply.started": "2025-05-12T10:04:29.129112Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "{f\"{k[0]},{k[1]}\": v for k, v in self.merges.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T10:02:32.790653Z",
     "iopub.status.busy": "2025-05-12T10:02:32.790266Z",
     "iopub.status.idle": "2025-05-12T10:02:32.852592Z",
     "shell.execute_reply": "2025-05-12T10:02:32.851748Z",
     "shell.execute_reply.started": "2025-05-12T10:02:32.790630Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
