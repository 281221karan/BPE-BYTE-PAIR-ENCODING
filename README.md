# BPE-BYTE-PAIR-ENCODING

If you are familiar with llm or every try to build a llm from scratch you may have a problem of tokenizer because the word level tokenizer is not very good for OOV tokens and the vocabulary size is too long which is not optimal so if you ever heard of BPE Tokenizer also know as sub word tokenizer which is used in every LLM and it can handle OOV tokens very easily and it is the most efficiend way to tokenize the text so here i made a BPE toknzer from scratch with easy to understand code and can be customized 
