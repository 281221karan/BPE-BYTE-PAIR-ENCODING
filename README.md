# BPE-BYTE-PAIR-ENCODING

If you are familiar with llm or ever try to build a llm from scratch you may have a problem of tokenizer because the word level tokenizer is not very good for OOV tokens and the vocabulary size is too long which is not optimal so if you ever heard of BPE Tokenizer also know as sub word tokenizer which is used in every LLM and it can handle OOV tokens very easily and it is the most efficiend way to tokenize the text so here i made a BPE toknzer from scratch with easy to understand code and can be customized 

This is the most easy to understand and efficient code to use, you can understand everything just from a screentshot here - https://github.com/281221karan/BPE-BYTE-PAIR-ENCODING/blob/main/Screenshot_1.png

  If you want to train the tokenizer from scratch see the screenshot and use the .train method.

  If you want to encode the text see the screenshot and use the .encode method.

  If you want to decode the integer see the screenshot and use the .decode method.


Thats all THERE.
